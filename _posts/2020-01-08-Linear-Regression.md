---
layout:     post
title:      Linear Regression
subtitle:   çº¿æ€§å›å½’
date:       2020-01-08
author:     Young
header-img: img/1*ScEjBlA2xNW4fflEu7DJCg.png
catalog: true
tags:
    - machine learning
    - python
---


## çº¿æ€§å›å½’çš„åŸç†
  - çº¿æ€§å›å½’çš„ä¸€èˆ¬å½¢å¼
    <p align="center">
    $$
    f(x)=\sum _{i=0}^{d}\theta_i x_i
    $$
    </p>
    å¦‚ä½•æ¥ç¡®å®š ğœƒ çš„å€¼ï¼Œä½¿å¾— ğ‘“(ğ‘¥) å°½å¯èƒ½æ¥è¿‘yçš„å€¼å‘¢ï¼Ÿå‡æ–¹è¯¯å·®æ˜¯å›å½’ä¸­å¸¸ç”¨çš„æ€§èƒ½åº¦é‡ï¼Œå³ï¼š
    <p align="center">
    $$
    J(\theta)=\frac{1}{2}\sum _{j=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})^2
    $$
    </p>

  - Why do we use the **Mean-Squared Loss(MSE)**?

  <p align="center">
    <img src="https://github.com/Julian-young/Julian-young.github.io/raw/dev-jiale/img/WX20200108-193842.png" style="zoom:80%" />
  </p>

## çº¿æ€§å›å½’æŸå¤±å‡½æ•°ã€ä»£ä»·å‡½æ•°ã€ç›®æ ‡å‡½æ•°

  [Objective function, cost function, loss function: are they the same thing?](https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing)
	
  <p align="center">
    <img src="https://github.com/Julian-young/Julian-young.github.io/raw/dev-jiale/img/WX20200108-200933@2x.png" style="zoom:80%" />
  </p>

## çº¿æ€§å›å½’çš„ä¼˜åŒ–æ–¹æ³•

### [Gradient Descent](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html)
  - Consider the 3-dimensional graph below in the context of a cost function. **Our goal is to move from the mountain in the top right corner (high cost) to the dark blue sea in the bottom left (low cost)**. The arrows represent the direction of steepest descent (negative gradient) from any given pointâ€“the direction that decreases the cost function as quickly as possible.

  <p align="center">
    <img src="https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent.png" style="zoom:80%" />
  </p>

  - **What is the objective of Gradient Descent?**
    <br>
    Gradient, in plain terms means slope or slant of a surface. So **gradient descent literally means descending a slope to reach the lowest point on that surface**. 
    <br>
    **Gradient descent is an iterative algorithm, that starts from a random point on a function and travels down its slope in steps until it reaches the lowest point of that function.**
    
  - **The Point of GD**
    <br>
    Minimizing any cost function means finding the deepest valley in that function. Keep in mind that, the cost function is used to monitor the error in predictions of an ML model. **So, the whole point of GD is to minimize the cost function**.

  <p align="center">
    <img src="https://miro.medium.com/max/1588/1*4VbVds8vD-CgAiOWTrs_Vw.png" style="zoom:80%" />
  </p>

  - **Learning rate**
    <br>
    The size of these steps is called the learning rate. **With a high learning rate we can cover more ground each step, but we risk overshooting the lowest point** since the slope of the hill is constantly changing. **With a very low learning rate, we can confidently move in the direction of the negative gradient** since we are recalculating it so frequently. **A low learning rate is more precise, but calculating the gradient is time-consuming**, so it will take us a very long time to get to the bottom.
  
  - Now letâ€™s run gradient descent using our new cost function. There are two parameters in our cost function we can control: *m* (weight) and *b* (bias). Since we need to consider the impact each one has on the final prediction, we need to use **partial derivatives**. We calculate the partial derivatives of the cost function with respect to each parameter and store the results in a gradient.
    - Given the cost function:
      <p align="center">
      $$
      f(m,b)=\frac{1}{N}\sum (y_i-(mx_i+b))^2
      $$
      </p>
      
    - The gradient can be calculated as:
    <p align="center">
    $$
    {f}'(m,b)=\begin{bmatrix}
    \frac{df}{dm}\\ 
    \frac{df}{db}
    \end{bmatrix}=\begin{bmatrix}
    \frac{1}{N}\sum -2x_i(y_i-(mx_i+b))\\ 
    \frac{1}{N}\sum -2(y_i-(mx_i+b))
    \end{bmatrix}
    $$
    </p>
    
    ```python 
    
      def update_weights(m, b, X, Y, learning_rate):
          m_deriv = 0
          b_deriv = 0
          N = len(X)
          for i in range(N):
              m_deriv += -2*X[i] * (Y[i] - (m*X[i] + b))
              b_deriv += -2*(Y[i] - (m*X[i] + b))

          //We subtract because the derivatives point in direction of steepest ascent
          m -= (m_deriv / float(N)) * learning_rate
          b -= (b_deriv / float(N)) * learning_rate

      return m, b
    ```

- **å¼€é”€åˆ†æ**
  <br>
  Suppose we have 10,000 data points and 10 features. The sum of squared residuals consists of as many terms as there are data points, so 10000 terms in our case. We need to compute the derivative of this function with respect to each of the features, so in effect we will be doing **10000 * 10 = 100,000 computations per iteration**. It is common to **take 1000 iterations**, in effect we have **100,000 * 1000 = 100000000 computations** to complete the algorithm. **That is pretty much an overhead and hence gradient descent is slow on huge data**.

  <p align="center">
    <img src="https://suniljangirblog.files.wordpress.com/2018/12/descent.png" style="zoom:80%" />
  </p>
  
### [Stochastic Gradient Descent (SGD)](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31)
  - **Where can we potentially induce randomness in our gradient descent algorithm??**
    <br>
    Yes, you might have guessed it right !! **It is while selecting data points at each step to calculate the derivatives**. SGD **randomly picks one data point from the whole data set at each iteration** to reduce the computations enormously.
### Mini-batch gradient descent
  - It is also common to **sample a small number of data points instead of just one point at each step** and that is called â€œmini-batchâ€ gradient descent. **Mini-batch tries to strike a balance between the goodness of gradient descent and speed of SGD**.
  - **æ¢¯åº¦ä¸‹é™æ³•çš„ç¼ºé™·ï¼šå¦‚æœå‡½æ•°ä¸ºéå‡¸å‡½æ•°ï¼Œæœ‰å¯èƒ½æ‰¾åˆ°çš„å¹¶éå…¨å±€æœ€ä¼˜å€¼ï¼Œè€Œæ˜¯å±€éƒ¨æœ€ä¼˜å€¼ã€‚**

### æœ€å°äºŒä¹˜æ³•çŸ©é˜µæ±‚è§£

- **The Least Squares Regression Line**
	<br>
  The Least Squares Regression Line is the line that **makes the vertical distance from the data points to the regression line as small as possible**. Itâ€™s called a â€œleast squaresâ€ because the best line of fit is one that **minimizes the variance (the sum of squares of the errors)**. 
<p align="center">
<img src="https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2014/11/least-squares-regression-line.jpg" style="zoom:100%" />
</p>

- ä»¥ä¼°è®¡æˆ¿ä»·ä¸ºä¾‹ï¼Œå‡è®¾çœŸå®ä¸–ç•Œé‡Œæˆ¿å­çš„é¢ç§¯ xä¸æˆ¿ä»· y çš„å…³ç³»æ˜¯çº¿æ€§å…³ç³»ï¼Œä¸”çœŸå®ä¸–ç•Œå­˜åœ¨æ— æ³•ä¼°è®¡çš„è¯¯å·® $\epsilon$ï¼Œä¹Ÿå°±æ˜¯ $y=w_0+w_1 x+\epsilon $ï¼Œæœ€å°äºŒä¹˜æ³•å°±æ˜¯è¦æ‰¾åˆ°ä½¿è¯¯å·® $\epsilon$ çš„å¹³æ–¹å’Œæœ€å°çš„ $w_0$ï¼Œ$w_1$å³å¯ã€‚
  <p align="center">
    $$
    y=Xw+\epsilon
    $$
  </p>
  <p align="center">
    $$
    \underset{w}{min} \ \epsilon^T \epsilon
    $$
  </p>
- **$\epsilon^T \epsilon$çš„å›¾åƒåƒä¸€ä¸ªç¢—**
  
  - å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¿™æ„å‘³ç€å­˜åœ¨ä¸€ä¸ªå…¨å±€æœ€ä½ç‚¹ï¼Œè¿™æ ·çš„å‡½æ•°å«åšå‡¸å‡½æ•°ï¼Œå¯ä»¥**ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥å¾—åˆ°å…¨å±€æœ€ä½ç‚¹å¯¹åº”çš„ w** ï¼Œè¿™é‡Œä¸å†èµ˜è¿°ï¼Œåªè®²**ç”¨å¾®ç§¯åˆ†ç›´æ¥æ±‚è§£ï¼ˆæœ€å°äºŒä¹˜æ³•ï¼‰**ã€‚
  
  <p align="center">
    <img src="https://iewaij.github.io/introDataScience/img/linRegContoursSSE.png" style="zoom:40%" />
  </p>
  
  <p align="center">
    <img src="https://github.com/Julian-young/Julian-young.github.io/raw/dev-jiale/img/WX20200109-155216@2x.png" style="zoom:80%" />
  </p>
  <p align="center">
    $$
    \nabla_w\epsilon^T \epsilon = 2X^TXw-2X^Ty
    $$
  </p>
  
  
  - å½“ $\nabla_w\epsilon^T \epsilon=0$æ—¶ï¼Œå¾—åˆ°ä½ç½®$\widehat{w} = (X^TX)^{-1}X^Ty$ï¼Œå³å¾—åˆ°å…¨å±€æœ€ä½ç‚¹å¯¹åº”çš„ w ã€‚
