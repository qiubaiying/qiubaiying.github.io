---
layout:     post
title:      Logistic Regression
subtitle:   é€»è¾‘å›å½’
date:       2020-01-11
author:     Young
header-img: img/0*xuDhGc5E9EVQdbQE.png
catalog: true
tags:
    - machine learning
    - python
---

### [Linear Regression vs Logistic Regression](https://www.javatpoint.com/linear-regression-vs-logistic-regression-in-machine-learning)

<p align="center">
  <img src="https://static.javatpoint.com/tutorial/machine-learning/images/linear-regression-vs-logistic-regression.png" style="zoom:80%" />
</p>

- **Main difference between them is how they are being used**
  <br>
  The Linear Regression is used for **solving Regression problems** whereas Logistic Regression is used for **solving the Classification problems**. 

  <table class="alt">
  <tbody><tr>
  	<th>Linear Regression</th>
  	<th>Logistic Regression</th>
  </tr>
  <tr>
    <td>Linear regression is used to predict the <b>continuous dependent variable</b> using a given set of independent variables.</td>
    <td>Logistic Regression is used to predict the <b>categorical dependent variable </b> using a given set of independent variables.</td>
  </tr>
  <tr>
    <td>Linear Regression is used for <b>solving Regression problem</b>.</td>
    <td>Logistic regression is used for <b>solving Classification problems</b>.</td>
  </tr>
  <tr>
    <td>In Linear regression, we predict the value of <b>continuous variables</b>.</td>
    <td>In logistic Regression, we predict the values of <b>categorical variables</b>.</td>
  </tr>
  <tr>
    <td>In linear regression, we find <b>the best fit line</b>, by which we can easily predict the output.</td>
    <td>In Logistic Regression, we find <b>the S-curve</b> by which we can classify the samples.</td>
  </tr>
  <tr>
    <td><b>Least square estimation method</b> is used for estimation of accuracy.</td>
    <td><b>Maximum likelihood estimation method</b> is used for estimation of accuracy.</td>
  </tr>
  <tr>
    <td>The output for Linear Regression must be a <b>continuous value</b>, such as price, age, etc.</td>
  	<td>The output of Logistic Regression must be a <b>Categorical value</b> such as 0 or 1, Yes or No, etc.</td>
  </tr>
  <tr>
  	<td>In Linear regression, it is required that relationship between dependent variable and independent variable must be linear.</td>
  	<td>In Logistic regression, it is not required to have the linear relationship between the dependent and independent variable.</td>
  </tr>
  <tr>
  	<td>In linear regression, there may be collinearity between the independent variables.</td>
  	<td>In logistic regression, there should not be collinearity between the independent variable.</td>
  </tr>
  </tbody></table>

### Theory

- **Decision boundary**
  <p align="center">
    <img src="https://ml-cheatsheet.readthedocs.io/en/latest/_images/logistic_regression_sigmoid_w_threshold.png" style="zoom:80%" />
  </p>
  
  - Suppose we have a generic training set $\lbrace \left(x^{(1)}, y^{(1)}\right),\left(x^{(2)}, y^{(2)}\right), \ldots,\left(x^{(m)}, y^{(m)}\right) \rbrace$, where $ğ‘¥(ğ‘š)$  is the input variable of the ğ‘š-th example, while $ğ‘¦(ğ‘š)$ is its output variable, ranging from 0 to 1. Finally we have the hypothesis function for logistic regression, $h_{\theta}(x)=\frac{1}{1+e^{-\theta^T x}}$.

### Interpretation

**[The cost function used in linear regression won't work here](https://www.internalpointers.com/post/cost-function-logistic-regression)**

- If we try to use the cost function of the linear regression in Logistic Regression $\sum^m_{i=1}(y^{(i)}-\frac{1}{1+e^{-\theta^T x}})^2$, then it would be of no use as it would **end up being a non-convex function with many local minimums**, in which it would be very **difficult to minimize the cost value and find the global minimum**.
  <p align="center">
    <img src="https://miro.medium.com/max/2096/1*dPXwswig8RTCAjstnUZNGQ.png" style="zoom:80%" />
  </p>

- Logistic regression cost function
  <p align="center">
  $$
  cost\left(h_{\theta}(x), y\right)=\left\{\begin{array}{ll}{-\log \left(h_{\theta}  
  (x)\right)} & {\text { if } y=1} \\ {-\log \left(1-h_{\theta}(x)\right)} & {\text { if } y=0}\end{array}\right.
  $$
  </p>
  å³ï¼Œ$cost\left(h_{\theta}(x), y\right)=-y \log \left(h_{\theta}(x)\right)-(1-y) \log \left(1-h_{\theta}(x)\right)$
  
- With the optimization in place, the logistic regression cost function can be rewritten as:
  <p align="center">
  $$
  \begin{aligned} J(\theta) &=\frac{1}{m} \sum_{i=1}^{m} 
  cost\left(h_{\theta}\left(x^{(i)}\right), y^{(i)}\right) \\ &=-\frac{1}{m}\left[\sum_{i=1}^{m} y^{(i)} \log    
  \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]   \end{aligned}
  $$
  </p>

- **$\frac{\partial}{\partial \theta_{j}} J(\theta)=\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}$[æ¨å¯¼è¿‡ç¨‹](https://stats.stackexchange.com/questions/278771/how-is-the-cost-function-from-logistic-regression-derivated)**
<p align="center">
  <img src="https://cdn.mathpix.com/snip/images/bx-W2Eom_W9ksP6UlnL9Cwl5nmbjv56GmoO-xdg1gZ8.original.fullsize.png" style="zoom:80%" />
</p>

- **$\frac{d}{d x} \sigma(x)=\sigma(x)(1-\sigma(x))$[æ¨å¯¼è¿‡ç¨‹](https://stats.stackexchange.com/questions/278771/how-is-the-cost-function-from-logistic-regression-derivated)**
<p align="center">
  <img src="https://cdn.mathpix.com/snip/images/aBPya8ZqTGWzuwbbIlYXBRGakE3Bum2JPUtiik-dUAM.original.fullsize.png" style="zoom:80%" />
</p>

### [é€»è¾‘å›å½’çš„åˆ†å¸ƒå¼å®ç°](https://blog.csdn.net/qq_32742009/article/details/81839071)

- **æŒ‰è¡Œå¹¶è¡Œ**
  <br>
  **å³å°†æ ·æœ¬æ‹†åˆ†åˆ°ä¸åŒçš„æœºå™¨ä¸Šå»ï¼ŒæŠŠæ•°æ®é›†æ‰“æ•£**ï¼ˆæ³¨ï¼šå³â€œåˆ’åˆ†â€ï¼Œè¦æ»¡è¶³ä¸é‡ã€ä¸æ¼ä¸¤ä¸ªæ¡ä»¶ï¼‰æˆ C å—ï¼Œ$\frac{\partial J}{\partial w}=\frac{1}{N} \sum_{k=1}^{K} \sum_{i \in C_{k}}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \times x^{(i)}$ï¼Œ ç„¶åå°†è¿™ C å—åˆ†é…åˆ°ä¸åŒçš„æœºå™¨ä¸Šå»ï¼Œåˆ™åˆ†å¸ƒå¼çš„è®¡ç®—æ¢¯åº¦ï¼Œåªä¸è¿‡æ˜¯**æ¯å°æœºå™¨éƒ½è®¡ç®—å‡ºå„è‡ªçš„æ¢¯åº¦ï¼Œç„¶åå½’å¹¶æ±‚å’Œå†æ±‚å…¶å¹³å‡**ã€‚
  - **ä¸ºä»€ä¹ˆå¯ä»¥è¿™ä¹ˆåšå‘¢ï¼Ÿ**
  <br>
  **æ¢¯åº¦ä¸‹é™å…¬å¼åªä¸ä¸Šä¸€ä¸ªæ›´æ–°æ‰¹æ¬¡çš„$\theta$åŠå½“å‰æ ·æœ¬æœ‰å…³**

- **æŒ‰åˆ—å¹¶è¡Œ**
  <br>
  æŒ‰åˆ—å¹¶è¡Œçš„æ„æ€å°±æ˜¯**å°†åŒä¸€æ ·æœ¬çš„ç‰¹å¾ä¹Ÿåˆ†å¸ƒåˆ°ä¸åŒçš„æœºå™¨ä¸­å»**ã€‚ä¸Šé¢çš„å…¬å¼ä¸ºé’ˆå¯¹æ•´ä¸ª$\theta$ï¼Œå¦‚æœæˆ‘ä»¬åªæ˜¯é’ˆå¯¹æŸä¸ªåˆ†é‡$\theta_{j}$ï¼Œå¯å¾—åˆ°å¯¹åº”çš„æ¢¯åº¦è®¡ç®—å…¬å¼ $ \frac{\partial J}{\partial w}=\frac{1}{N} \sum_{i=1}^{N}\left(h_{\theta_j}\left(x_{j}^{(i)}\right)-y^{(i)}\right) \times x _{j}^{(i)} $ ï¼Œå³ä¸å†æ˜¯ä¹˜ä»¥æ•´ä¸ª $x_n$ï¼Œè€Œæ˜¯ä¹˜ä»¥ $x_n$ å¯¹åº”çš„åˆ†é‡ $x_{n,j}$ï¼Œæ­¤æ—¶å¯ä»¥å‘ç°ï¼Œ**æ¢¯åº¦è®¡ç®—å…¬å¼ä»…ä¸$x_n$ä¸­çš„ç‰¹å¾æœ‰å…³ç³»**ï¼Œæˆ‘ä»¬å°±å¯ä»¥**å°†ç‰¹å¾åˆ†å¸ƒåˆ°ä¸åŒçš„è®¡ç®—ä¸Šï¼Œåˆ†åˆ«è®¡ç®—$\theta_{j}$å¯¹åº”çš„æ¢¯åº¦ï¼Œæœ€åå½’å¹¶ä¸ºæ•´ä½“çš„$\theta$ï¼Œå†æŒ‰è¡Œå½’å¹¶åˆ°æ•´ä½“çš„æ¢¯åº¦æ›´æ–°ã€‚**
