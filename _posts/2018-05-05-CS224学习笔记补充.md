---
layout:     post
title:      CS224学习笔记补充
subtitle:   信息熵
date:       2018-05-05
author:     徐清华
header-img: img/post-bg-ios9-web.jpg
catalog: 	 true
tags:
    - CS224
    - 信息熵
---
1.Intuition
--
&emsp;熵是对信息量大小的一种描述,是香农在信息论中提出的概念,为了了解熵的概念的提出,有必要对信息论有一定的了解.
&emsp;信息,是一个非常模糊的概念,涵盖很广也很杂乱,那么如何去描述信息量的大小呢?在描述的过程中难免会产生很多歧义.例如,假设天气预报一共有4种可能的情况,包括:rainy,sunny,cloudy,foggy.单独拿rainy这种情况出来看,就会产生歧义,因为rainy 既可以看做是4种    况包含的信息量显然是不同的.从这个例子我们可以得出两个结论:
&emsp; 1.信息的描述是相对的,同一个描述可以从不同的层次上去理解,就能得到不同的信息内涵,相应的,其对应的信息量大小自然也不一定相同.
&emsp; 2.从直观上可以理解,信息量的大小和选择的多少是有关的,和可选择的各种情况是有关的,这也就引出了下文中使用变量的方式描述的方法.
----
&emsp; 我们不妨把思路先固定在数学层面上,进行一步简单的建模---使用变量的概念去描述现实世界,如果能够表示出一个变量的信息量,就可以进一步表示出现实世界中的各种信息量的大小.变量根据取值范围的不同可以分为连续变量和离散变量.而每一个取值(每一种可能情况)都是具有一定的信息量的,变量的总信息量可以看做是各个情况的信息量的平均值.每一种情况的信息量叫自信息,自信息的平均值就是熵.(不理解没有关系,后面还会详细讲解)
&emsp;而具体又该怎么去求每一种情况包含的信息量呢?以简单的抛硬币的实验为例子,假如该硬币并不平衡,有99%的概率是正面,只有1%的概率是反面.那么,如果进行一次实验,结果是正面,我们会说这次实验没有多少价值,也就是包含的信息量没有多少;反之,如果是反面,那么我们会说这次实验很成功,结果是包含了很多信息的.从这里我们就能看出,每一种情况的信息量的值是和该情况的取值概率呈反比的,因为概率越小说明猜到这种情况的可能行越小,而信息量的大小是相对的,因此我们可以选择符合反比关系的任意一种表达式作为信息量的表示.香农选择了使用指数形式便于计算(~~同时也基于比特的考虑--这么大的概率代表了多大的吃惊程度呢?如果所有情况都平均以这个概率出现,那么需要最少需要猜多少次才能猜对呢?~~),即
![image.png](https://upload-images.jianshu.io/upload_images/12011882-7da98dcde2d42fba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;有了自信息的表述 ,就可以很容易的得出一个变量的总体信息量的表述,只需要对所有可能取值的自信息取平均值即可(连续变量需要使用积分的方式求期望),也就得出了信息熵的最终形式.熵的单位是比特,在很多情况下可以利用是或否的问题推论出结果是哪一种情况,而这些是或否的问题都可以时候用一个比特进行表述.
![image.png](https://upload-images.jianshu.io/upload_images/12011882-3a919dc9c456b75c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

2.Entropy
--
>The measure of information entropy associated with each possible data value is the negative [logarithm](https://en.wikipedia.org/wiki/Logarithm "Logarithm") of the [probability mass function](https://en.wikipedia.org/wiki/Probability_mass_function "Probability mass function") for the value. Thus, when the data source has a lower-probability value (i.e., when a low-probability event occurs), the event carries more "information" ("surprisal") than when the source data has a higher-probability value. The amount of information conveyed by each event defined in this way becomes a [random variable](https://en.wikipedia.org/wiki/Random_variable "Random variable") whose expected value is the information entropy. Generally, *entropy* refers to disorder or uncertainty, and the definition of entropy used in information theory is directly analogous to the [definition used](https://en.wikipedia.org/wiki/Entropy_(statistical_thermodynamics) "Entropy (statistical thermodynamics)") in [statistical thermodynamics](https://en.wikipedia.org/wiki/Statistical_thermodynamics "Statistical thermodynamics"). The concept of information entropy was introduced by [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon "Claude Shannon") in his 1948 paper "[A Mathematical Theory of Communication](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication "A Mathematical Theory of Communication")".<sup>[[1]](https://en.wikipedia.org/wiki/Entropy_(information_theory)#cite_note-shannonPaper-1)</sup>

&emsp;上文引用自维基百科,可以看出,信息熵是通过描述变量的不确定性来描述信息的多少.这些在前文中都已经详细介绍过,在此不再赘述.稍微多提一下信息量的单位比特.目前为止总体的思路应该很明确:
&emsp;想要描述信息量-->描述变量的信息量---->描述变量的不确定性的大小--->每一种情况出现的惊讶程度有多高,再求和---->每一种情况要猜的话,按照**最佳策略**(概率大的先猜)需要猜几次---->需要问多少个是或否的问题才能猜出来--->需要多少比特进行表述

3.Cross Entropy
--
![image.png](https://upload-images.jianshu.io/upload_images/12011882-5a7714ce6683b9ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;交叉熵可以看做是信息熵的一种应用.根据维基百科上的定义可以看出,交叉熵可以用来表示两个概率分布的相近程度.概率分布的相似程度可以使用距离进行表示,在数理统计学上有很多可以使用的方案.包括KL距离,F距离等.在交叉熵中借用了信息熵.前文中也提到了信息熵是在**最佳策略**的情况下得到的平均计数次数,所谓最佳策略指的是概率大的情况就优先猜;显然,任何一种其他的策略都只能增加猜测的次数,此外,策略和最佳策略相差越大,则猜的次数越多.这个性质就可以用来计算两个概率分布的相似程度.
![image.png](https://upload-images.jianshu.io/upload_images/12011882-52a819d59307d87c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;上述公式中,如果把p(x)改为q(x)即为信息熵的公式,也就是最佳策略的情况p(x)是人工分配的概率,如果和真实的概率不同,就会使得交叉熵的数值增大

4.Applications in Machine Learning
--
&emsp;信息熵的应用是非常广泛的,尤其是在机器学习领域当中.下面的内容主要翻译自[这篇博客](https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32)
#### Bayesian learning
&emsp;所有机器学习方法的目的都是为了减少信息熵.我们想要对某件事做出预测,我们就需要对我们的预测结果有一定的信心.而信息熵恰恰给我们提供一个可以度量这种信息程度大小的工具.在贝叶斯方法中,先验分布总是具有一个相对跨度比较广的概率密度函数(比较均匀),体现了在获取观察值之前,我们对这个参数一无所知,不确定性很高.当有数据的时候,信息熵就会减少,并且后验分布会在参数最可能的取值附近形成极大值.

#### Decision tree learning
在决策树的学习过程中,信息熵被用来构建树.构造一棵树需要从根节点开始出发,通过选择最有最大的熵增益(Entropy Gain)的属性作为根节点,可以更快的完成树的构建.属性有不同的取值,对应着不同的情况,所以有着不同的信息熵.
#### Classification
&emsp;交叉熵是Logistic Regression 以及神经网络的标准损失函数(包括二类划分和多类划分).通常,p是用来表示真实的分布,q用来表示模型求出来的分布.以二分类的Logistic Regression分类为例子,模型最终给给出每一个输入x对应各个标签的概率 q_(y=1) = ŷ and q_(y=0) = 1 − ŷ  .这个可以被更加精确的写成 q ∈ {ŷ, 1 − ŷ}的形式. 因此每一个训练样本的交叉熵都可以表示为
![image.png](https://upload-images.jianshu.io/upload_images/12011882-b7047cbdbb112628.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;通常情况下,我们会使用整个样本集的交叉熵之和作为损失函数,如下
![image.png](https://upload-images.jianshu.io/upload_images/12011882-b45dc9f67b1b0c6f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### KL divergence
&emsp;这是一个和交叉熵有着很密切联系的概念,是机器学习中另外一个常用的相似度度量的工具.在贝叶斯推理中, DKL(p||q)用来表示从先验分布到后验分布的转变造成的信息的损失.可以使用信息熵和交叉熵求得表达形式,由上文我们可以得知,交叉熵表示不采用最佳策略时的信息熵,那么KL值实际上是不采用最佳策略时和最佳策略的差值.
![image.png](https://upload-images.jianshu.io/upload_images/12011882-987d36d6a4f5cf3e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

5.Conclusion
--
&emsp;总之,对于信息熵的认识有了更加进一步的认识,也打开了更多新世界的大门.下一步将会简要的介绍贝叶斯方法,因为从本次学习的过程中,认识到贝叶斯方法和信息熵是密切联系在一起的,所以有必要对其进行一网打尽.

