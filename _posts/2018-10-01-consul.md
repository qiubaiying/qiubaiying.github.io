---
title: Consul on Kubernetes
date: 2018-09-20 23:30:50
tags: k8s
---

### Consul简介
> Consul 是 HashiCorp 公司推出的开源工具，用于实现分布式系统的服务发现与配置。与其他分布式服务注册与发现的方案，Consul的方案更“一站式”，内置了服务注册与发现框 架、分布一致性协议实现、健康检查、Key/Value存储、多数据中心方案，不再需要依赖其他工具（比如ZooKeeper等）。使用起来也较 为简单。Consul使用Go语言编写，因此具有天然可移植性(支持Linux、windows和Mac OS X)；安装包仅包含一个可执行文件，方便部署，与Docker等轻量级容器可无缝配合 。
&nbsp;
### 首先理解一下consul的架构
> consul的节点可以分为server与client，都是通过consul agent命令来定义具体角色，client可以理解成consul集群的代理，开发者使用consul的SDK就可以无缝使用consul的各种功能，无需关心端口、consul集群的IP（k8s中的ClusterIP）等。

![cluster.png](http://q7mj5531m.bkt.clouddn.com/cluster.png)
### 服务发现比较Consul vs Zookeeper vs Etcd vs Eureka

| Feature      | Consul               | zookeeper           |etcd           |euerka           |
| -------------|--------------------- |---------------------|---------------|-----------------|
| 服务健康检查 |服务状态，内存，硬盘等|(弱)长连接，keepalive|连接心跳       |可配支持         |
| 多数据中心   |支持                  |—                    |—              |—                |
| kv存储服务   |支持                  |支持                 |支持           |—                |
| 一致性       |raft                  |paxos                |raft           |—                |
| 使用接口     |支持http和dns         |客户端               |http/grpc      |http             |
| watch支持    |全量/支持long polling |支持                 |支持longpolling|支持longpolling  |
| 自身监控     |metrics               |—                    |metrics        |metrics          |
| 安全         |acl/https             |acl                  |https支持（弱）|—                |
| SpringCloud  |支持                  |支持                 |支持           |支持             |

&nbsp;
> 以下内容来源于网络

- 服务的健康检查
  Euraka 使用时需要显式配置健康检查支持；Zookeeper,Etcd 则在失去了和服务进程的连接情况下显示任务不健康，而Consul相对更为详细点，比如内存是否已使用了90%，文件系统的空间是不是快不足了。
- 多数据中心支持
  Consul 通过 WAN 的 Gossip 协议，完成跨数据中心的同步。
- KV存储服务
  除了 Eureka ,其他几款都能够对外支持kv的存储服务，提供存储服务也能够较好的转化为动态配置服务。
- 多语言能力与对外提供服务的接入协议
  Zookeeper的跨语言支持较弱，Euraka 一般通过 sidecar的方式提供多语言客户端的接入支持。Etcd还提供了Grpc的支持。Consul除了标准的Restapi,还提供了DNS的支持。
- Watch的支持（客户端观察到服务提供者变化）
  Zookeeper 支持服务器端推送变化，Eureka 2.0(正在开发中)也计划支持。 Eureka 1,Consul,Etcd则都通过长轮询的方式来实现变化感知。
- 自身集群的监控
  除了 Zookeeper ,其他几款都默认支持metrics，运维人员可以搜集并监控这些度量信息。
- 安全
  Consul,Zookeeper 支持ACL，另外 Consul,Etcd 支持安全通道https.

&nbsp;

### 一，环境介绍
* 操作系统：CentOS Linux release 7.5.1804 (Core)
* 内核版本：3.10.0-862.11.6.el7.x86_64
* Docker版本：17.03.2-ce
* k8s版本：v1.10.0
* k8s集群：3个master 3个etcd 3个node （kubeadm部署，网络使用flannel-v0.10.0-amd64）
* 分布式存储：4节点GlusterFS，创建volume，提供持久化存储
&nbsp;
### 二，部署
&nbsp;
1：设置cfssl环境
```
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
chmod +x cfssl_linux-amd64
mv cfssl_linux-amd64 /usr/local/bin/cfssl
chmod +x cfssljson_linux-amd64
mv cfssljson_linux-amd64 /usr/local/bin/cfssljson
chmod +x cfssl-certinfo_linux-amd64
mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo
export PATH=/usr/local/bin:$PATH
```
&nbsp;
2: 下载consul的二进制文件，https://www.consul.io/downloads.html   下载Linux版的，下下来之后unzip，并把可执行文件放到/usr/bin目录下。
&nbsp;
3：clone文件
```
git clone https://github.com/zhangzhaorui/consul-cluster-on-kubernetes.git && cd ./consul-cluster-on-kubernetes
```
&nbsp;
4：Consul集群中成员之间的RPC通信使用TLS进行加密
- 通过以下命令初始化CA证书：
```
cfssl gencert -initca ca/ca-csr.json | cfssljson -bare ca
```
- 使用以下命令创建 TLS 证书 和 私有密钥:
```
cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca/ca-config.json \
  -profile=default \
  ca/consul-csr.json | cfssljson -bare consul
```
- 执行完以上命令之后，在当前目录你应该看到以下证书文件:
```
ca-key.pem
ca.pem
consul-key.pem
consul.pem
```
&nbsp;
5：生成 Consul Gossip加密密钥（secret）和Consul Cluster的配置文件（configmap）
- Consul Cluster使用共享的加密密钥进行加密通信，使用以下命令生成加密密钥:
```
GOSSIP_ENCRYPTION_KEY=$(consul keygen)
```
- 将Gossip加密密钥和TLS证书存储在kubernetes的Secret中:
```
kubectl create secret generic consul \
  --from-literal="gossip-encryption-key=${GOSSIP_ENCRYPTION_KEY}" \
  --from-file=ca.pem \
  --from-file=consul.pem \
  --from-file=consul-key.pem
```
- 将Consul集群使用的配置文件存储在kubernetes的configmap中:
```
kubectl create configmap consul --from-file=configs/server.json
```
&nbsp;
6：配置GlusterFS存储，生成pvpvc，提供Consul-cluster的持久化存储（PS：之后这块会在补充一下GFS的搭建，使用和怎样作为后端存储，我现在没有用heketi）
- 在gluserfs集群中创建3个consul volume，ip地址请根据自己的情况设置。
```
gluster volume create consul-0 replica 2 transport tcp 172.18.130.21:/opt/consul-0 172.18.130.22:/opt/consul-0 172.18.130.23:/opt/consul-0 172.18.130.24:/opt/consul-0 force
gluster volume create consul-1 replica 2 transport tcp 172.18.130.21:/opt/consul-1 172.18.130.22:/opt/consul-1 172.18.130.23:/opt/consul-1 172.18.130.24:/opt/consul-1 force
gluster volume create consul-2 replica 2 transport tcp 172.18.130.21:/opt/consul-2 172.18.130.22:/opt/consul-2 172.18.130.23:/opt/consul-2 172.18.130.24:/opt/consul-2 force

gluster volume start consul-0
gluster volume start consul-1
gluster volume start consul-2
gluster volume quota consul-0 enable
gluster volume quota consul-1 enable
gluster volume quota consul-2 enable
gluster volume quota consul-0 limit-usage / 200MB
gluster volume quota consul-1 limit-usage / 200MB
gluster volume quota consul-2 limit-usage / 200MB
gluster volume set consul-0 performance.cache-size 10MB
gluster volume set consul-1 performance.cache-size 10MB
gluster volume set consul-2 performance.cache-size 10MB
gluster volume set consul-0 performance.io-thread-count 16
gluster volume set consul-1 performance.io-thread-count 16
gluster volume set consul-2 performance.io-thread-count 16
gluster volume set consul-0 network.ping-timeout 10
gluster volume set consul-1 network.ping-timeout 10
gluster volume set consul-2 network.ping-timeout 10
gluster volume set consul-0 cluster.self-heal-daemon on
gluster volume set consul-1 cluster.self-heal-daemon on
gluster volume set consul-2 cluster.self-heal-daemon on
gluster volume set consul-0 cluster.heal-timeout 300
gluster volume set consul-1 cluster.heal-timeout 300
gluster volume set consul-2 cluster.heal-timeout 300
gluster volume set consul-0 performance.write-behind-window-size 10MB
gluster volume set consul-1 performance.write-behind-window-size 10MB
gluster volume set consul-2 performance.write-behind-window-size 10MB
```
7：创建pvpvc
```
kubectl apply -f pvpvc/pv.yaml
kubectl apply -f pvpvc/pvc.yaml
```
![pvpvc.png](http://q7mj5531m.bkt.clouddn.com/pvpvc.png)
&nbsp;

8：创建service
```
kubectl create -f services/consul-service.yaml
```
![svc.png](http://q7mj5531m.bkt.clouddn.com/svc.png)
&nbsp;

9：通过StatefulSet部署Consul集群
```
kubectl create -f statefulsets/consul-statefulsets.yaml 
```
![pod.png](http://q7mj5531m.bkt.clouddn.com/pod.png)
&nbsp;

### 三，验证
&nbsp;
1，查看pod的log
```
kubectl logs -f consul-0
```
* 正确的日志是这样
* 如果三个pod都在running状态的，但是kubectl logs -f consul-的时候报错no cluster leader说明三节点的consul集群没有启动成功，需要定位原因
![log.png](http://q7mj5531m.bkt.clouddn.com/log.png)

&nbsp;

2，进入到pod内部，执行consul members
![member.png](http://q7mj5531m.bkt.clouddn.com/member.png)

&nbsp;

3，登陆webui，可以找一台服务器安装一个nginx，把podIP+port8500暴露出去，就可以访问consul的ui了，也可以通过traefik-ingress
![webui.png](http://q7mj5531m.bkt.clouddn.com/webui.png)




